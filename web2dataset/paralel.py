# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/paralel.ipynb (unless otherwise specified).

__all__ = ['ParalelDownload']

# Cell
import time
from threading import Thread
from typing import List
from docarray import Document, DocumentArray
from docarray.array.memory import DocumentArrayInMemory # hack to fix the cyclic issue

from multiprocessing.pool import ThreadPool
from rich.progress import Progress

from .downloader import Downloader

# Cell
class ParalelDownload:
    def __init__(
        self,
        path: str,
        downloader_cls: type[Downloader],
        num_workers: int,
        dataset_fn: str = "dataset.bin",
        *args,
        **kwargs,
    ):
        """Parallelize donwloading
        path: folder in which to save the files
        downloader_cls: the class of Downloader you want to parellize
        num_workers: the number of worker to use.
        dataset_fn: name of the file in which to save the docarray dataset, by default dataset.bin
        silence: to silence the logging and the progress bar
        *args: args to pass to the downloader init function
        *kwargs: kwargs to pass to the downloader init function
        """

        self.path = path
        self.dataset_fn = dataset_fn
        self.num_workers = num_workers
        self.downloader_cls = downloader_cls
        self._get_downloader = lambda : downloader_cls(path=path, *args, **kwargs)



    def download(
        self,
        queries: List[str],
        n_item: int,
        silence: bool = False,
    ):
        with Progress(*self.downloader_cls.get_default_column(),disable=silence) as progress:

            queries = [(task_id, query) for task_id, query in enumerate(queries)]
            for _, query in queries:
                progress.add_task(f"Scrapping {query} ...", total=n_item)

            downloader_list= []
            with ThreadPool(processes  = self.num_workers) as pool:

                def task(t_query):
                    (task_id, query) = t_query
                    downloader = self._get_downloader()
                    downloader_list.append(downloader)
                    downloader._download(query, n_item, progress, task_id)

                pool.map(task, queries)

            with open(f"{self.path}/{self.dataset_fn}", "wb") as f:
                self.docs = DocumentArray().empty()
                for d in downloader_list:
                    self.docs.extend(d.docs)
                f.write(self.docs.to_bytes())