{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8998a0eb-2647-4dea-abf5-ff6690b457f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp paralel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00de67c2-331a-4946-8e8e-9ed94964b261",
   "metadata": {},
   "source": [
    "# Pararel Downloading\n",
    "\n",
    "This define a paralel executor for the downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22126b4-b78c-4ce3-ba5e-38111af9a1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f667b66-c79b-4ae1-b90e-93b21a00c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab140d-844f-42c6-9157-d04b2ebf0b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import time\n",
    "from threading import Thread\n",
    "from typing import List\n",
    "from docarray import Document, DocumentArray\n",
    "from docarray.array.memory import DocumentArrayInMemory # hack to fix the cyclic issue\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from rich.progress import Progress\n",
    "\n",
    "from web2dataset.downloader import Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35191be-fc63-4592-9e83-88efd6a96853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ParalelDownload:\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str,\n",
    "        downloader_cls: type[Downloader],\n",
    "        num_workers: int,\n",
    "        dataset_fn: str = \"dataset.bin\",\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Parallelize donwloading\n",
    "        path: folder in which to save the files\n",
    "        downloader_cls: the class of Downloader you want to parellize\n",
    "        num_workers: the number of worker to use.\n",
    "        dataset_fn: name of the file in which to save the docarray dataset, by default dataset.bin\n",
    "        silence: to silence the logging and the progress bar\n",
    "        *args: args to pass to the downloader init function\n",
    "        *kwargs: kwargs to pass to the downloader init function\n",
    "        \"\"\"\n",
    "\n",
    "        self.path = path\n",
    "        self.dataset_fn = dataset_fn\n",
    "        self.num_workers = num_workers\n",
    "        self.downloader_cls = downloader_cls\n",
    "        self._get_downloader = lambda : downloader_cls(path=path, *args, **kwargs)\n",
    "\n",
    "        \n",
    "\n",
    "    def download(\n",
    "        self,\n",
    "        queries: List[str],\n",
    "        n_item: int,\n",
    "        silence: bool = False,\n",
    "    ):\n",
    "        with Progress(*self.downloader_cls.get_default_column(),disable=silence) as progress:\n",
    "\n",
    "            queries = [(task_id, query) for task_id, query in enumerate(queries)]\n",
    "            for _, query in queries:\n",
    "                progress.add_task(f\"Scrapping {query} ...\", total=n_item)\n",
    "            \n",
    "            downloader_list= []\n",
    "            with ThreadPool(processes  = self.num_workers) as pool:\n",
    "            \n",
    "                def task(t_query):\n",
    "                    (task_id, query) = t_query\n",
    "                    downloader = self._get_downloader()\n",
    "                    downloader_list.append(downloader)\n",
    "                    downloader._download(query, n_item, progress, task_id)\n",
    "\n",
    "                pool.map(task, queries)\n",
    "\n",
    "            with open(f\"{self.path}/{self.dataset_fn}\", \"wb\") as f:\n",
    "                self.docs = DocumentArray().empty()\n",
    "                for d in downloader_list:\n",
    "                    self.docs.extend(d.docs)\n",
    "                f.write(self.docs.to_bytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20974181-0fc4-4c53-b064-07b2cb24f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = \"/tmp/test_paralel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef75f38-8b5b-42c1-b710-5fb661f9a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree(test_folder, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab112b8-e8f4-4074-881d-523859a5f119",
   "metadata": {},
   "source": [
    "let's define first a downloader to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0754562a-4a24-48a8-991f-270fd45482ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "class BasicDownloader(Downloader):\n",
    "    def _download(self, query: str, n_item: int, progress: Progress, task_id: int = 0):\n",
    "        time.sleep(0.5)\n",
    "        self.docs.extend(\n",
    "            (Document(tag={\"origin\": \"https://www.google.fr\"}) for _ in range(n_item))\n",
    "        )\n",
    "        progress.update(task_id, advance=n_item)\n",
    "        time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfb52cf-1c62-4315-861a-89ad2131dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "paralel_down = ParalelDownload(\n",
    "    f\"{test_folder}/my_search\", BasicDownloader, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd12d5c7-bbe7-452b-8df8-4300086e185c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c60cb133e64ff0a04c501c65953784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 382 ms, sys: 20.4 ms, total: 402 ms\n",
      "Wall time: 2.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "paralel_down.download([\"a\", \"b\", \"c\", \"d\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b9d6ff-2432-4eb8-adaa-73657de65924",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(paralel_down.docs) == 12"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
