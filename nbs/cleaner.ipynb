{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef3c64-5ed7-4ba9-9da7-81419b3d92f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67349d9-d059-40fd-8c23-f72a9b3f83aa",
   "metadata": {},
   "source": [
    "# Cleaner\n",
    "\n",
    "> The clearners are the last but not least blocks of web2dataset. Their goal is to purge and clean the dataset.\n",
    "\n",
    "Example of cleaner (not yet implemented):\n",
    "\n",
    "* delete double (based on hash)\n",
    "* delete image with low resolution\n",
    "* ethic base purger, how to ?\n",
    "\n",
    "There are two kind of cleaner, the one that work on metadata that are called before downloading the image and the one that work on image and are called after\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed6fdad-d86e-48c5-a724-f6545391568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9449780d-0577-47ee-8f8b-4d11c5879e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b344f2-80c9-4f91-b9bd-ac37015fc134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from abc import ABC, abstractmethod\n",
    "from functools import wraps\n",
    "from typing import List\n",
    "\n",
    "from web2dataset.document import Document\n",
    "from web2dataset.utils_data import get_metadata_path, load_docs,get_images_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7418a9db-d9ee-41f1-904b-37014a2c39f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8eb7bd-f1da-4267-93a6-405b62feb0c6",
   "metadata": {},
   "source": [
    "# MetaDataCleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0013013-0d3c-465f-b85d-7e0b8c737be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MetaDataCleanerError(ValueError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5c87c1-78c2-45bc-ac7b-b8dd3b6255dd",
   "metadata": {},
   "source": [
    "a cleaner should delete docs not create them, so we verify than we did not create new docs with this wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c576b143-a95a-4c0b-bcea-a39408bbac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def check_no_docs_creation(f):\n",
    "    @wraps(f)\n",
    "    def wrapper(self, docs: List[Document]) -> List[Document]:\n",
    "        new_docs = f(self, docs)\n",
    "        if len(new_docs) > len(docs):\n",
    "            raise MetaDataCleanerError(\n",
    "                f\"the cleaner should not create more docs than originaly. There were before {len(docs)} docs and there are now {len(new_docs)} docs\"\n",
    "            )\n",
    "        return new_docs\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a4ee6c-dd5d-462b-8e0f-fdada9be799d",
   "metadata": {},
   "source": [
    "Here is the abstract class for the meta data cleaner. It only operate on documents not images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091f0262-3d27-4307-ad28-8a2cfc59965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MetaDataCleaner(ABC):\n",
    "    @abstractmethod\n",
    "    @check_no_docs_creation\n",
    "    def clean(self, docs: List[Document]) -> List[Document]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d77afae-5889-4c32-bcff-c89da5f605b3",
   "metadata": {},
   "source": [
    "here is a basic cleaner that is mainly used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d16abd3-09e5-4f08-95c6-56c4e1eb8cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class IdentityCleaner:\n",
    "    @check_no_docs_creation\n",
    "    def clean(self, docs: List[Document]) -> List[Document]:\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ee6f3f-6597-461f-9438-00dc67ca7fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    Document(origin=\"\", image_url=\"https://image/bike\"),\n",
    "    Document(origin=\"\", image_url=\"https://image/bike\"),\n",
    "    Document(origin=\"\", image_url=\"https://image/bmx\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482fd352-670b-4f36-aeb1-c56a50b05865",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = IdentityCleaner()\n",
    "docs = cleaner.clean(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526e65dd-7578-4669-b61d-0233177d8e8b",
   "metadata": {},
   "source": [
    "## Duplicate cleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa25d5-1bd6-4383-b878-9ac82f823829",
   "metadata": {},
   "source": [
    "This MetaDataDuplicateCleaner delete any duplicate, i.e document with the same src image to avoid downloading twice the same image.\n",
    "It is different from the ImageDuplicateCleaner, will delete two identical image after the donwload, this images could come from two different sources be still be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298713fa-5aaf-4e08-bbb1-f9459ca52f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DuplicateCleaner(MetaDataCleaner):\n",
    "    @check_no_docs_creation\n",
    "    def clean(self, docs: List[Document]) -> List[Document]:\n",
    "        url_doc = {\n",
    "            doc.image_url: doc for doc in docs\n",
    "        }  # first we create a dict with image url as key because we want to keep only one doc per image_rul\n",
    "        return list(url_doc.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fdcdfc-e7c9-45ac-8dbe-2312c87945d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    Document(origin=\"\", image_url=\"https://image/bike\"),\n",
    "    Document(origin=\"\", image_url=\"https://image/bike\"),\n",
    "    Document(origin=\"\", image_url=\"https://image/bmx\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39206222-a04b-4235-99d0-0540a234ec41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = [doc.image_url for doc in docs]\n",
    "len(url), len(set(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de54ae-1f7b-40c1-925b-bb8050fe167e",
   "metadata": {},
   "source": [
    "as we can see in this list of doc there are 3 url but only two of them are different. Let's fix it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bd3fa7-cb52-499f-b333-b0ea2a53ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = DuplicateCleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3079f90e-b875-470c-b537-77b075cb3b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs2 = cleaner.clean(docs)\n",
    "url = [doc.image_url for doc in docs2]\n",
    "\n",
    "assert len(url) == len(set(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7792dd25-a171-431d-9345-30a72df7fc74",
   "metadata": {},
   "source": [
    "# ImageCleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c188167-6d2b-4350-a56a-2bf09ed90dbe",
   "metadata": {},
   "source": [
    "Image cleaner operatire directly on image and are called after the downloading step wheras the MetaDataCleaner are called after the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f77f56-b4c7-443b-8ec5-885ae1cca3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ImageCleaner(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class to delete\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: str):\n",
    "        self.path = path\n",
    "\n",
    "    @abstractmethod\n",
    "    def clean(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84115008-3048-4dbb-b706-0997114829e5",
   "metadata": {},
   "source": [
    "## Duplicate image cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affa6806-54f9-460c-a8a2-26104729ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuplicateImageCleaner(ImageCleaner):\n",
    "    \"\"\"\n",
    "    Delete duplicate image by computing the hash of each image and delete doublon\n",
    "    args:\n",
    "        path: str. A string containing the path to the folder (where we saved a search and download images)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init_(self, path: str):\n",
    "        super().__init__(path)\n",
    "        self.documents = load_docs(self.path)\n",
    "        self.image_path = get_images_path(self.path)\n",
    "\n",
    "    def _extract_hash(self):\n",
    "        pass\n",
    "        s\n",
    "    def clean(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b844a995-81ad-4c9c-b8ef-15549404fd0d",
   "metadata": {},
   "source": [
    "let's try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f19a7e-cf7d-46d9-af8e-9375646c20e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from web2dataset.downloader import BasicDownloader\n",
    "from web2dataset.searcher import BasicSearcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a09524e-93dc-4219-8ee1-d569ac97ef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = \"/tmp/my_search_test_cleaner\"\n",
    "if os.path.isdir(path_test):\n",
    "    shutil.rmtree(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cf047b-c971-4f23-b591-c83d9bf20fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = BasicSearcher(\"\", 2)\n",
    "searcher.search()\n",
    "searcher.save(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18f6e7-c81a-4993-b09f-c36e292954ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader = BasicDownloader(path_test)\n",
    "downloader.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f672c-99c9-4f34-9b82-352671e5495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = DuplicateImageCleaner(path_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
